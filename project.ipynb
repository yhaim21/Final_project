{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the Data Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -U scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import pickle\n",
    "from tqdm import  tqdm\n",
    "import shutil\n",
    "\n",
    "pd.options.mode.chained_assignment = None\n",
    "pd.options.display.min_rows = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import socket\n",
    "import os\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basketball-refrence.com scraper:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping\n",
      " ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ \n",
      " https://www.basketball-reference.com/leagues/NBA_2016_games.html\n",
      "https://www.basketball-reference.com/leagues/NBA_2017_games.html\n",
      "https://www.basketball-reference.com/leagues/NBA_2018_games.html\n",
      "https://www.basketball-reference.com/leagues/NBA_2019_games.html\n",
      "https://www.basketball-reference.com/leagues/NBA_2020_games.html\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(4-4) | Getting Game URLs: 2019-20: 100%|██████████| 4/4 [00:33<00:00,  8.32s/it]\n",
      "Scraping Games: 2016-17: 100%|██████████| 861/861 [06:30<00:00,  2.20it/s]\n",
      "Scraping Games: 2017-18: 100%|██████████| 825/825 [36:06<00:00,  2.63s/it] \n",
      "Scraping Games: 2018-19: 100%|██████████| 822/822 [32:38<00:00,  2.38s/it]    \n",
      "Scraping Games: 2019-20: 100%|██████████| 688/688 [05:28<00:00,  2.09it/s]\n"
     ]
    }
   ],
   "source": [
    "class BbrefScraper:\n",
    "    def __init__(self, season_start_links, scrape_type):\n",
    "        self.scrape_type = scrape_type\n",
    "        self.season_start_links = season_start_links\n",
    "        self.base_url = 'https://www.basketball-reference.com'\n",
    "        #change to your own directoty\n",
    "        self.data_path = '/Users/yoavhaim/Desktop/Programming/project'\n",
    "        if not os.path.exists(self.data_path):\n",
    "            os.mkdir(self.data_path)\n",
    "            os.mkdir(f'{self.data_path}/pickles')\n",
    "            os.mkdir(f'{self.data_path}/bbref-files')\n",
    "\n",
    "        self.team_dictionary = {\n",
    "            'Atlanta Hawks': 'Atl', 'Boston Celtics': 'Bos', 'Brooklyn Nets': 'Bkn', 'Charlotte Hornets': 'Cha',\n",
    "            'Chicago Bulls': 'Chi', 'Cleveland Cavaliers': 'Cle', 'Dallas Mavericks': 'Dal', 'Denver Nuggets': 'Den',\n",
    "            'Detroit Pistons': 'Det', 'Golden State Warriors': 'GSW', 'Houston Rockets': 'Hou', 'Indiana Pacers': 'Ind',\n",
    "            'Los Angeles Lakers': 'LAL', 'Los Angeles Clippers': 'LAC', 'Memphis Grizzlies': 'Mem', 'Miami Heat': 'Mia',\n",
    "            'Milwaukee Bucks': 'Mil', 'Minnesota Timberwolves': 'Min', 'New Orleans Pelicans': 'Nor', 'New York Knicks': 'NYK',\n",
    "            'Oklahoma City Thunder': 'OKC', 'Orlando Magic': 'Orl', 'Philadelphia 76ers': 'Phi', 'Phoenix Suns': 'Pho',\n",
    "            'Portland Trail Blazers': 'Por', 'Sacramento Kings': 'Sac', 'San Antonio Spurs': 'SAS', 'Toronto Raptors': 'Tor',\n",
    "            'Utah Jazz': 'Uta', 'Washington Wizards': 'Was'\n",
    "        }\n",
    "\n",
    "    def scrape_stats(self):\n",
    "        print(f'Scraping\\n','~' * 50, '\\n', '\\n'.join(self.season_start_links))\n",
    "        #get the url for each month in the season\n",
    "        self.get_months()\n",
    "        # print(self.month_url_dict)\n",
    "\n",
    "        #get the game links for each game within each month\n",
    "        self.get_game_links()\n",
    "        # print(self.full_game_urls)\n",
    "\n",
    "        #scrape each table in each game and save to dataframe\n",
    "        self.get_game_stats()\n",
    "\n",
    "    def get_game_stats(self):\n",
    "        # self.full_game_urls = pickle.load(open(f'{self.data_path}/pickles/GameLinks.p', 'rb'))\n",
    "        for season, game_links in self.full_game_urls.items():\n",
    "            pieces = []\n",
    "            pbar = tqdm(game_links, desc = f'Scraping Games: {season}')\n",
    "            # total = 0\n",
    "            for link in pbar:\n",
    "                response = requests.get(link)\n",
    "                soup = BeautifulSoup(response.text, 'html.parser')\n",
    "                table_df = self.get_table_info(soup, link)\n",
    "                pieces.append(table_df)\n",
    "                # total += 1\n",
    "                # if total == 50:\n",
    "                #     break\n",
    "            full_season_df = pd.concat(pieces)\n",
    "            full_season_df.to_csv(f'{self.data_path}/bbref-files/{season}.csv', index = False)\n",
    "\n",
    "    def get_table_info(self, soup, link):\n",
    "        columns = ['Player', 'Date', 'Team', 'Against', 'Home', 'MP', 'FG', 'FGA', 'FG%', '3P', '3PA', '3P%', 'FT', 'FTA', 'FT%', 'ORB',\n",
    "                   'DRB', 'TRB', 'AST', 'STL', 'BLK', 'TOV', 'PF', 'PTS', '+/-']\n",
    "        # column_2 = ['TS%', 'eFG%',\n",
    "        #            'FTr', 'ORB%', 'DRB%', 'TRB%', 'AST%', 'STL%', 'BLK%', 'TOV%', 'USG%', 'ORtg', 'DRtg', 'BPM']\n",
    "        header = soup.findAll('h1')[0].text.split(',')\n",
    "        date = ''.join(header[-2:]).strip()\n",
    "        teams = header[0].split('at')\n",
    "        # away_team = teams[0].strip()\n",
    "        # home_team = teams[1][:teams[1].find('Box Score')].strip()\n",
    "        away_team = teams[0].strip()\n",
    "        home_team = teams[-1][:teams[-1].find('Box Score')].strip()\n",
    "        if home_team == '':\n",
    "            home_team = teams[-2][:teams[-2].find('Box Score')].strip()\n",
    "        tables = soup.findAll('tbody')\n",
    "        #get unqiue players\n",
    "        player_dict = {}\n",
    "        away_idx = [0]\n",
    "        home_idx = [8]\n",
    "        for idx, table in enumerate(tables):\n",
    "            # print(idx)\n",
    "            # print(table)\n",
    "            # print('~'*50)\n",
    "            if idx not in away_idx + home_idx:\n",
    "                continue\n",
    "            if idx in away_idx:\n",
    "                team = away_team\n",
    "                opp = home_team\n",
    "                home = 0\n",
    "            else:\n",
    "                team = home_team\n",
    "                opp = away_team\n",
    "                home = 1\n",
    "            for row in table:\n",
    "                try:\n",
    "                    name = row.findAll('th')[0].text\n",
    "                    if name == 'Reserves':\n",
    "                        continue\n",
    "                    player_dict[name] = [name, date, team, opp, home]\n",
    "                except AttributeError:\n",
    "                    continue\n",
    "        for idx, table in enumerate(tables):\n",
    "            if idx not in away_idx + home_idx:\n",
    "                continue\n",
    "            for row in table:\n",
    "                try:\n",
    "                    name = row.findAll('th')[0].text\n",
    "                    cols = row.findAll('td')\n",
    "                    if len(cols) == 0:\n",
    "                        continue\n",
    "                    cols = [i.text.strip() for i in cols]\n",
    "                    for c in cols:\n",
    "                        player_dict[name].append(c)\n",
    "                except AttributeError:\n",
    "                    continue\n",
    "        game_df_pieces = []\n",
    "        for name, l in player_dict.items():\n",
    "            row_dict = {col:value for col, value in zip(columns, l)}\n",
    "            row_df = pd.DataFrame(row_dict, index = [0])\n",
    "            game_df_pieces.append(row_df)\n",
    "        game_df = pd.concat(game_df_pieces)\n",
    "        game_df['GameLink'] = [link for i in range(len(game_df))]\n",
    "        # game_df.to_csv('Tester.csv', index = False)\n",
    "        # game_df.to_csv('Tester.csv', index = False)\n",
    "        return game_df\n",
    "\n",
    "    def get_game_links(self):\n",
    "        self.full_game_urls = {}\n",
    "        pbar = tqdm((enumerate(self.month_url_dict.items())), total = len(self.month_url_dict))\n",
    "        for idx, (season, month_url_list) in pbar:\n",
    "            pbar.set_description(f'({idx+1}-{len(self.month_url_dict)}) | Getting Game URLs: {season}')\n",
    "            season_game_urls = []\n",
    "            for month_url in month_url_list:\n",
    "                response = requests.get(month_url)\n",
    "                soup = BeautifulSoup(response.text, 'html.parser')\n",
    "                time.sleep(1)\n",
    "                table = soup.findAll('tbody')\n",
    "                # print(table)\n",
    "                game_links = table[0].findAll('a', href = True)\n",
    "                for idx, link in enumerate(game_links):\n",
    "                    if (idx + 1) % 4 != 0:\n",
    "                        continue\n",
    "                    if link.text.strip() != 'Box Score':\n",
    "                        continue\n",
    "                    full_url = f'{self.base_url}{link[\"href\"]}'\n",
    "                    season_game_urls.append(full_url)\n",
    "\n",
    "            self.full_game_urls[season] = season_game_urls\n",
    "        pbar.close()\n",
    "        # pickle.dump(self.full_game_urls, open(f'{self.data_path}/pickles/GameLinks.p', 'wb'))\n",
    "        del self.month_url_dict\n",
    "\n",
    "    def get_months(self):\n",
    "        if self.scrape_type == 0:\n",
    "            months = ['october', 'november', 'december', 'january', 'february', 'march', 'april', 'may', 'june']\n",
    "        elif self.scrape_type == 1:\n",
    "            months = ['november', 'december', 'january', 'february', 'march', 'april', 'may', 'june']\n",
    "        elif self.scrape_type == 2:\n",
    "            months = ['december', 'january', 'february', 'march', 'april', 'may', 'june']\n",
    "        elif self.scrape_type == 3:\n",
    "            months = ['october-2019', 'november', 'december', 'january', 'february', 'march', 'july', 'august', 'september', 'october-2020']\n",
    "        elif self.scrape_type == 4:\n",
    "            months = ['december', 'january', 'february', 'march']\n",
    "        self.month_url_dict = {}\n",
    "        for season_start_link in self.season_start_links:\n",
    "            response = requests.get(season_start_link)\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            season = soup.find('h1').text.split(' ')[0].strip()\n",
    "            if os.path.exists(f'{self.data_path}/bbref-files/{season}.csv'):\n",
    "                continue\n",
    "            year = f'20{season[season.find(\"-\")+1:]}'\n",
    "            season_month_list = []\n",
    "            for month in months:\n",
    "                base_url = f'https://www.basketball-reference.com/leagues/NBA_{year}_games-{month}.html'\n",
    "                season_month_list.append(base_url)\n",
    "            self.month_url_dict[season] = season_month_list\n",
    "        # print(self.month_url_dict[season])\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    season_list_3 = [\n",
    "        'https://www.basketball-reference.com/leagues/NBA_2020_games.html'\n",
    "    ]\n",
    "    season_list_2 = [\n",
    "        'https://www.basketball-reference.com/leagues/NBA_2012_games.html',\n",
    "    ]\n",
    "    season_list_1 = [\n",
    "        'https://www.basketball-reference.com/leagues/NBA_2005_games.html',\n",
    "        'https://www.basketball-reference.com/leagues/NBA_2006_games.html',\n",
    "    ]       #doesnt work with current method\n",
    "\n",
    "    season_list_0 = [\n",
    "        #'https://www.basketball-reference.com/leagues/NBA_2001_games.html',\n",
    "        #'https://www.basketball-reference.com/leagues/NBA_2002_games.html',\n",
    "        #'https://www.basketball-reference.com/leagues/NBA_2003_games.html',\n",
    "        #'https://www.basketball-reference.com/leagues/NBA_2004_games.html',\n",
    "        #'https://www.basketball-reference.com/leagues/NBA_2007_games.html',\n",
    "        #'https://www.basketball-reference.com/leagues/NBA_2008_games.html',\n",
    "        #'https://www.basketball-reference.com/leagues/NBA_2009_games.html',\n",
    "        #'https://www.basketball-reference.com/leagues/NBA_2010_games.html',\n",
    "        #'https://www.basketball-reference.com/leagues/NBA_2011_games.html',\n",
    "        #'https://www.basketball-reference.com/leagues/NBA_2013_games.html',\n",
    "        #'https://www.basketball-reference.com/leagues/NBA_2014_games.html',\n",
    "        #'https://www.basketball-reference.com/leagues/NBA_2015_games.html',\n",
    "        'https://www.basketball-reference.com/leagues/NBA_2016_games.html',\n",
    "        'https://www.basketball-reference.com/leagues/NBA_2017_games.html',\n",
    "        'https://www.basketball-reference.com/leagues/NBA_2018_games.html',\n",
    "        'https://www.basketball-reference.com/leagues/NBA_2019_games.html',\n",
    "        'https://www.basketball-reference.com/leagues/NBA_2020_games.html'\n",
    "\n",
    "    ]\n",
    "    season_dict = {0: season_list_0, 1: season_list_1, 2: season_list_2, 3: season_list_3}\n",
    "    total = len(season_list_0) + len(season_list_1) + len(season_list_2) + len(season_list_3)\n",
    "    # print(f'Total Games: {total}')\n",
    "    # for scrape_type, season_list in season_dict.items():\n",
    "        # nba_stat_scraper = BbrefScraper(season_list, scrape_type = scrape_type)\n",
    "        # nba_stat_scraper.scrape_stats()\n",
    "\n",
    "    season_list = [\n",
    "       'https://www.basketball-reference.com/leagues/NBA_2021_games.html'\n",
    "    ]\n",
    "\n",
    "    nba_stat_scraper = BbrefScraper(season_list_0, scrape_type = 4)\n",
    "    nba_stat_scraper.scrape_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# clean the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import shutil\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2016-17.csv',\n",
       " '2006-07.csv',\n",
       " '2014-15.csv',\n",
       " '2012-13.csv',\n",
       " '2002-03.csv',\n",
       " '2007-08.csv',\n",
       " '2000-01.csv',\n",
       " '2019-20.csv',\n",
       " '2010-11.csv',\n",
       " '2017-18.csv',\n",
       " '2015-16.csv',\n",
       " '2009-10.csv',\n",
       " '2008-09.csv',\n",
       " '2018-19.csv',\n",
       " '2020-21.csv',\n",
       " '2013-14.csv',\n",
       " '2003-04.csv',\n",
       " '2001-02.csv']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "data_dir = '/Users/yoavhaim/Desktop/Programming/Project/bbref-files'\n",
    "cleaned_data_dir = '/Users/yoavhaim/Desktop/Programming/Project/cleaned'\n",
    "ignore = ['cleaned']\n",
    "file_paths = [i for i in os.listdir(data_dir) if i not in ignore]\n",
    "file_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2016-17.csv\n",
      "2006-07.csv\n",
      "2014-15.csv\n",
      "2012-13.csv\n",
      "2002-03.csv\n",
      "2007-08.csv\n",
      "2000-01.csv\n",
      "2019-20.csv\n",
      "2010-11.csv\n",
      "2017-18.csv\n",
      "2015-16.csv\n",
      "2009-10.csv\n",
      "2008-09.csv\n",
      "2018-19.csv\n",
      "2020-21.csv\n",
      "2013-14.csv\n",
      "2003-04.csv\n",
      "2001-02.csv\n"
     ]
    }
   ],
   "source": [
    "team_names = []\n",
    "for csv_path in file_paths:\n",
    "    print(csv_path)\n",
    "    csv_path = f'{data_dir}/{csv_path}'\n",
    "    df = pd.read_csv(csv_path)\n",
    "    team_names += df.Team.unique().tolist()\n",
    "    team_names += df.Against.unique().tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'s', 'Charlotte Hornets', 'Philadelphia 76ers', 'Miami H', 'Los Angeles Clippers', 'Indiana Pacers', 'Memphis Grizzlies', 'Portland Trail Blazers', 'Oklahoma City Thunder', 'tle SuperSonics', 'e Warriors', 'New Orleans/Oklahoma City Hornets', 'Se', 'Vancouver Grizzlies', 'Milwaukee Bucks', 'Charlotte Bobc', 'Sacramento Kings', 'New Jersey Nets', 'Phoenix Suns', 'Denver Nuggets', 'New Orleans Pelicans', 'Chicago Bulls', 'Brooklyn Nets', 'New York Knicks', 'Detroit Pistons', 'Orlando Magic', 'Minnesota Timberwolves', 'San Antonio Spurs', 'Houston Rockets', 'Golden St', 'Dallas Mavericks', 'New Orleans Hornets', 'Cleveland Cavaliers', 'Atlanta Hawks', 'Utah Jazz', 'Toronto Raptors', 'Washington Wizards', 'Los Angeles Lakers', 'Boston Celtics', 'Miami He'}\n"
     ]
    }
   ],
   "source": [
    "print(set(team_names))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_teams(x):\n",
    "    if x in ['Se', 'tle SuperSonics']:\n",
    "        return 'Seattle SuperSonics'\n",
    "    elif x in ['Golden St', 'e Warriors']: \n",
    "        return 'Golden State Warriors'\n",
    "    elif x in ['Miami H', 'Miami He']: \n",
    "        return 'Miami Heat'\n",
    "    elif x in ['s']:\n",
    "        return 'Charlotte Bobcats'\n",
    "    else: \n",
    "        return x\n",
    "def convert_mp(x):\n",
    "    if type(x) == float: \n",
    "        return None\n",
    "    elif x in ['Did Not Play', 'Did Not Dress', 'Not With Team', 'Player Suspended']:\n",
    "        return None\n",
    "    else:\n",
    "        x = x.split(':')\n",
    "        x = float('.'.join(x))\n",
    "        return x\n",
    "def convert_fg(x):\n",
    "    if type(x) == float:\n",
    "        return None\n",
    "    elif x in ['Did Not Play', 'Did Not Dress', 'Not With Team', 'Player Suspended']:\n",
    "        return None\n",
    "    else:\n",
    "        return int(x)\n",
    "columns = ['Player', 'Date', 'Team', 'Against', 'Home', 'MP', 'FG', 'FGA', \n",
    "           'FG%', '3P', '3PA', '3P%', 'FT', 'FTA', 'FT%', 'ORB', 'DRB',\n",
    "           'TRB', 'AST', 'STL', 'BLK', 'TOV', 'PF', 'PTS', '+/-', 'GameLink']\n",
    "df_team_cleaned = {}\n",
    "for csv_path in file_paths:\n",
    "    new_path = f'{data_dir}/{csv_path}'\n",
    "    df = pd.read_csv(new_path)\n",
    "    df['Team'] = df.Team.map(convert_teams)\n",
    "    df['Against'] = df.Against.map(convert_teams)\n",
    "    df['Date'] = pd.to_datetime(df.Date)\n",
    "    df['MP'] = df.MP.map(convert_mp)\n",
    "    df['FG'] = df.FG.map(convert_fg)\n",
    "    df_team_cleaned[csv_path] = df\n",
    "    df.to_csv(f'{cleaned_data_dir}/{csv_path}', index = False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Total Data is clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# converting the data to players:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import pickle\n",
    "from tqdm import  tqdm\n",
    "import shutil\n",
    "\n",
    "pd.options.mode.chained_assignment = None\n",
    "pd.options.display.min_rows = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Getting CSV for each player..: 100%|██████████| 1960/1960 [00:43<00:00, 45.53it/s]\n"
     ]
    }
   ],
   "source": [
    "#CREATE THE COMBINED DF AND SPLIT PLAYERS OUT TO THEIR OWN FILES\n",
    "\n",
    "bbreff_dir = '/Users/yoavhaim/Desktop/Programming/Project/cleaned'\n",
    "\n",
    "pieces = []\n",
    "for csv_name in os.listdir(bbreff_dir):\n",
    "    full_path = f'{bbreff_dir}/{csv_name}'\n",
    "    season = csv_name[:csv_name.find('.csv')]\n",
    "    df = pd.read_csv(full_path)\n",
    "    df['season'] = [season for _ in range(len(df))]\n",
    "    pieces.append(df)\n",
    "\n",
    "full_df = pd.concat(pieces)\n",
    "\n",
    "\n",
    "original_player_dir = '/Users/yoavhaim/Desktop/Programming/Project/players/data'\n",
    "pbar = tqdm(full_df.Player.unique(), desc = 'Getting CSV for each player..')\n",
    "for player in pbar:\n",
    "    subset_df = full_df[full_df.Player == player]\n",
    "    subset_df.to_csv(f'{original_player_dir}/{player}.csv', index = False)\n",
    "pbar.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'items'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-ff759b1c1603>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0mfile_paths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mignore\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfile_paths\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m     \u001b[0mcleaned_df_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfill_na\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-36-ff759b1c1603>\u001b[0m in \u001b[0;36mfill_na\u001b[0;34m(df_dict)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mnew_df_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'Player'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Date'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'FDP'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'FDSal'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;31m#         df = df[[i for i in columns]]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mdf_none\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFDP\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misnull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFDSal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misnull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'items'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaned_df_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
